{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761cb4ef-b9a4-41a7-a22d-d890b39e76c3",
   "metadata": {},
   "source": [
    "Поработаем над датасетом из соревнования [UtkMl](https://www.kaggle.com/competitions/utkmls-twitter-spam-detection-competition/overview) про определение спама в твитах.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a896f68d-babe-41e8-bbc7-2db43e5d625b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34741a82-f113-4c4e-a2a0-e1916a6a66b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data.drop('Unnamed: 7', axis=1, inplace=True)  \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d3b50-153b-4c68-a69f-c3ad2cc5ee41",
   "metadata": {},
   "source": [
    "Почистим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86ad3a8-abb6-4743-a8aa-73f873d8b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5737d15-509f-40d7-823d-45ffacdda788",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d550c-0bc1-471b-a08d-4d35e1b4002e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data[data.Type != \"South Dakota\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab2eee7-7243-4faf-9f0a-8ac4b1667ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['Type'] = data['Type'].apply(lambda x: 0 if x == 'Quality' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd0b14-775a-4cfc-bbd2-5c1154cbf1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.drop('location', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c0d8d-dbda-45e3-850f-c9cbd78ad09e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = data['Type']\n",
    "X = data.drop('Type', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727447a1-2710-4580-a92f-9ac04c42f65a",
   "metadata": {},
   "source": [
    "Давайте построим графики. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5b26e-b563-4b2e-bb7c-c15f99c89ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "for ax, feat in zip(axes.flat, X.select_dtypes(include=np.number).columns):\n",
    "    sns.kdeplot(X[feat], ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be9c77-b741-45ab-9e17-a7561fbaffe5",
   "metadata": {},
   "source": [
    "Видим, что а) ни один из признаков не имеет нормального распределения б) для небинарных признаков самое частое значение - 0. Скорее всего, наиболее важную информацию несут сами твиты (мы намеренно не будем удалять ссылки и хештеги, потому что они, скорее всего, вообще сделают самый важный вклад)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc90414-bd86-4011-bc92-a40c4aa3dad4",
   "metadata": {},
   "source": [
    "Теперь нам необходимо обработать наши признаки. Часть из них - текстовые, и их нужно обработать отдельно. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7c1bf-baaf-4b68-b0a0-f65c114d16d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering(choice_transformer, choice_ngrams):\n",
    "    # числовые характеристики нормализуем: imputer обрабатывает наны, scaler масштабирует\n",
    "    numeric_features = ['following', 'followers', 'actions', 'is_retweet']\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "    # текстовые характеристики обрабатываем: либо tf-idf, либо мешок слов\n",
    "    text_features = 'Tweet'\n",
    "    if choice_transformer == 'tfidf':\n",
    "        text_transformer = TfidfVectorizer(ngram_range=choice_ngrams, tokenizer=word_tokenize, stop_words='english')\n",
    "    else:\n",
    "        text_transformer = CountVectorizer(ngram_range=choice_ngrams, tokenizer=word_tokenize, stop_words='english')\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"txt\", text_transformer, text_features),\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b37cf-f615-46da-8393-9240d979870f",
   "metadata": {},
   "source": [
    "Обучим модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a077f4c-e9df-497d-b669-4f6cff558f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test.drop('Id', axis=1, inplace=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c173ff-55fd-4793-9296-7b04b00b3191",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modelfit(model):\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    \n",
    "    ypredtest = model.predict(Xtest)\n",
    "    ypredtrain = model.predict(Xtrain)\n",
    "    \n",
    "    print(accuracy_score(ytest, ypredtest), accuracy_score(ytrain, ypredtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afc7d71-0164-4cb4-b116-90c5adcdf6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf2a88-6878-4100-aca6-f83b57c4b460",
   "metadata": {},
   "source": [
    "TF-IDF unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068df0f5-4aad-4366-b05a-ee18669b6f08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = feature_engineering('tfidf', (1, 1))\n",
    "\n",
    "clfLR = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "clfSVC = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", SVC())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b81995c-6b6c-47f4-afed-5971ddd515f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(clfLR)\n",
    "modelfit(clfSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4faad9-aa15-40c4-909a-03427bed66b7",
   "metadata": {},
   "source": [
    "TF-IDF bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea26a0-c231-4d3f-8ec3-cc539696a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = feature_engineering('tfidf', (2, 2))\n",
    "\n",
    "clfLR = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "clfSVC = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", SVC())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe658882-94c8-4c6a-83b9-8bc4b4c83747",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(clfLR)\n",
    "modelfit(clfSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570b527f-5a2c-42b8-9582-97bf1007d75a",
   "metadata": {},
   "source": [
    "BOW unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae12084d-c5a6-44ea-9c5f-270241fbf806",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = feature_engineering('bow', (1, 1))\n",
    "\n",
    "clfLR = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "clfSVC = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", SVC())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa8b2e2-bd77-4e9a-8451-28b616bef0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(clfLR)\n",
    "modelfit(clfSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc25af-b0cc-498f-9196-59b2e79dd72a",
   "metadata": {},
   "source": [
    "BOW bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec833a10-2a43-4cf8-bf2b-fe349d66fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = feature_engineering('bow', (2, 2))\n",
    "\n",
    "clfLR = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "clfSVC = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", SVC())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006367c-376f-407a-bbf2-00e9f94378c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(clfLR)\n",
    "modelfit(clfSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e0ab7-9193-4441-afb5-b3ece33cc7ac",
   "metadata": {},
   "source": [
    "BOW Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e033784-38c3-4e9b-b559-49d512fbf8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = feature_engineering('bow', (1, 1))\n",
    "\n",
    "bagging = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", BaggingClassifier())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0633012-cd4a-48cd-8e4b-f4724d702961",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef959f39-9e0b-4bfa-aa85-d366aba252f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = feature_engineering('bow', (2, 2))\n",
    "\n",
    "bagging = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", BaggingClassifier())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffca4ee-beb6-4923-99c3-b9ea6d82b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2758b30-a7ad-4098-89ee-6930be9f9092",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725da3f0-b789-4ea4-a923-7ca7f7d99c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = feature_engineering('bow', (2, 2))\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "modelfit(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050542b-723b-4854-bbcc-4f3233f67725",
   "metadata": {},
   "source": [
    "Проверка только на твитах (чтобы посмотреть, что влияет сильнее всего)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2eabc7-6a14-4a85-9796-be80baa1df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(3, 3), tokenizer=word_tokenize, stop_words='english')\n",
    "bow = vec.fit_transform(Xtrain['Tweet'])\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(bow, ytrain)\n",
    "ypredtest = clf.predict(vec.transform(Xtest['Tweet']))\n",
    "print(classification_report(ypredtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668fb1ba-154e-4738-b3b8-ea2c3ff1e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(vec.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c53652-5373-4a39-abc2-c810359f7180",
   "metadata": {},
   "source": [
    "Проверим гипотезу о том, что ссылки сильнее всего влияют на определение спама"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18298f1d-dff0-4d92-ad14-6a4d297a54f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(data['Tweet'])\n",
    "nums = []\n",
    "\n",
    "for num, w in enumerate(tweets):\n",
    "    if 't.co' in w:\n",
    "        nums.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3268106-e8dc-4b31-a544-50252afcb110",
   "metadata": {},
   "outputs": [],
   "source": [
    "spamham = list(data['Type'])\n",
    "needed = []\n",
    "for i, w in enumerate(spamham):\n",
    "    if i in nums:\n",
    "        needed.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12227072-5779-4db1-9a70-054694c2e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ffa5c-4976-4cad-a2d7-a9a0136c7a82",
   "metadata": {},
   "source": [
    "Оказывается, подавляющее большинство текстов со ссылками определено как спам."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c8f0a-31b1-4030-9250-6b46937b6368",
   "metadata": {},
   "source": [
    "А теперь давайте посмотрим, как можно было ту же задачу решить с помощью эмбеддингов Doc2Vec из gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe54d55-80f3-4ef7-83e1-8fb65fd32c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394f25b-4416-4c94-9eac-e544b9cb6c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn import utils\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df8abc-58c4-4263-8970-08b8f656a5e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734942d-3f23-454d-9978-6f1d67944692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    '''gensim сам токенизировать не умеет, поэтому нам придется сделать это за него'''\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "train, test = train_test_split(data[['Tweet', 'Type']], test_size=0.3, random_state=42)\n",
    "\n",
    "# соберем специальный объект класса TaggedDocument, чтобы D2V работал\n",
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['Tweet']), tags=[r.Type]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['Tweet']), tags=[r.Type]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22258bfc-6d37-4170-af79-911ca044f878",
   "metadata": {},
   "source": [
    "Воспользуемся тем, что у большинства современных процессоров больше одного ядра..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb43bf50-547c-47b8-9173-085198af0371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a2c9f-33ff-49d5-bab3-bbbe928ca1f3",
   "metadata": {},
   "source": [
    "Обучим модельку DBoW (Distributed Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90869a91-564d-40cc-80d6-fee5bd73a764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample=0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c071a3e0-4089-489d-abc0-88337c995c66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e9adc0-23c5-4934-86ae-990d0dff1caf",
   "metadata": {},
   "source": [
    "Напишем аналог transform для D2V:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74264ac-dc40-437d-8b03-d7954cea9822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947130a0-6eb2-4595-8084-e88a58a99f38",
   "metadata": {},
   "source": [
    "Обучим банальную логистическую регрессию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa49355-ceaf-4aaf-af8e-b7c0471e2f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "logreg = LogisticRegression(solver='liblinear', n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cdc5e0-6a8a-4ade-80fd-bffd745d260d",
   "metadata": {},
   "source": [
    "С другими алгоритмами можете побаловаться сами. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85639436-6b32-467f-ac27-37e11f404c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
